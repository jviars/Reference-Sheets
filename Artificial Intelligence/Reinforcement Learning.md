# Reinforcement Learning Reference Sheet

## Fundamental Concepts

### Markov Decision Process (MDP)
1. **Components**
   ```
   State space (S)
   Action space (A)
   Transition function P(s'|s,a)
   Reward function R(s,a,s')
   Discount factor Œ≥ ‚àà [0,1]
   ```

2. **Properties**
   ```
   Markov Property: P(s‚Çú‚Çä‚ÇÅ|s‚Çú,a‚Çú,s‚Çú‚Çã‚ÇÅ,...,s‚ÇÄ) = P(s‚Çú‚Çä‚ÇÅ|s‚Çú,a‚Çú)
   Stationarity: Transition probabilities don't change over time
   ```

### Value Functions
1. **State Value Function**
   ```
   V^œÄ(s) = ùîºœÄ[Œ£‚ÇñŒ≥·µèr‚Çú‚Çä‚Çñ‚Çä‚ÇÅ|s‚Çú=s]
   ```

2. **Action Value Function**
   ```
   Q^œÄ(s,a) = ùîºœÄ[Œ£‚ÇñŒ≥·µèr‚Çú‚Çä‚Çñ‚Çä‚ÇÅ|s‚Çú=s,a‚Çú=a]
   ```

3. **Optimal Value Functions**
   ```
   V*(s) = max_œÄ V^œÄ(s)
   Q*(s,a) = max_œÄ Q^œÄ(s,a)
   ```

### Bellman Equations
1. **Bellman Expectation**
   ```
   V^œÄ(s) = Œ£‚ÇêœÄ(a|s)[R(s,a) + Œ≥Œ£‚Çõ'P(s'|s,a)V^œÄ(s')]
   Q^œÄ(s,a) = R(s,a) + Œ≥Œ£‚Çõ'P(s'|s,a)Œ£‚Çê'œÄ(a'|s')Q^œÄ(s',a')
   ```

2. **Bellman Optimality**
   ```
   V*(s) = max‚Çê[R(s,a) + Œ≥Œ£‚Çõ'P(s'|s,a)V*(s')]
   Q*(s,a) = R(s,a) + Œ≥Œ£‚Çõ'P(s'|s,a)max‚Çê'Q*(s',a')
   ```

## Classical Algorithms

### Dynamic Programming
1. **Policy Iteration**
   ```
   1. Policy Evaluation:
      Iterate V^œÄ(s) = Œ£‚ÇêœÄ(a|s)[R(s,a) + Œ≥Œ£‚Çõ'P(s'|s,a)V^œÄ(s')]
   
   2. Policy Improvement:
      œÄ'(s) = argmax‚Çê[R(s,a) + Œ≥Œ£‚Çõ'P(s'|s,a)V^œÄ(s')]
   ```

2. **Value Iteration**
   ```
   Iterate:
   V(s) ‚Üê max‚Çê[R(s,a) + Œ≥Œ£‚Çõ'P(s'|s,a)V(s')]
   ```

### Monte Carlo Methods
1. **First Visit MC**
   ```
   For each state s:
   V(s) = average of returns following first visit to s
   ```

2. **Every Visit MC**
   ```
   For each state s:
   V(s) = average of returns following every visit to s
   ```

### Temporal Difference Learning
1. **TD(0)**
   ```
   V(s‚Çú) ‚Üê V(s‚Çú) + Œ±[r‚Çú‚Çä‚ÇÅ + Œ≥V(s‚Çú‚Çä‚ÇÅ) - V(s‚Çú)]
   ```

2. **Q-Learning**
   ```
   Q(s‚Çú,a‚Çú) ‚Üê Q(s‚Çú,a‚Çú) + Œ±[r‚Çú‚Çä‚ÇÅ + Œ≥max‚ÇêQ(s‚Çú‚Çä‚ÇÅ,a) - Q(s‚Çú,a‚Çú)]
   ```

3. **SARSA**
   ```
   Q(s‚Çú,a‚Çú) ‚Üê Q(s‚Çú,a‚Çú) + Œ±[r‚Çú‚Çä‚ÇÅ + Œ≥Q(s‚Çú‚Çä‚ÇÅ,a‚Çú‚Çä‚ÇÅ) - Q(s‚Çú,a‚Çú)]
   ```

## Deep Reinforcement Learning

### Deep Q-Network (DQN)
1. **Key Components**
   ```
   Experience Replay Buffer
   Target Network
   Double DQN
   Dueling Architecture
   ```

2. **Loss Function**
   ```
   L = (r + Œ≥max‚Çê'Q(s',a';Œ∏‚Åª) - Q(s,a;Œ∏))¬≤
   ```

### Policy Gradient Methods
1. **REINFORCE**
   ```
   ‚àáJ(Œ∏) = ùîºœÄ[‚àálog œÄ(a|s;Œ∏)G]
   where G = Œ£‚ÇñŒ≥·µèr‚Çú‚Çä‚Çñ‚Çä‚ÇÅ
   ```

2. **Actor-Critic**
   ```
   ‚àáJ(Œ∏) = ùîºœÄ[‚àálog œÄ(a|s;Œ∏)A^œÄ(s,a)]
   where A^œÄ(s,a) = Q^œÄ(s,a) - V^œÄ(s)
   ```

### Advanced Algorithms

1. **Proximal Policy Optimization (PPO)**
   ```
   L^CLIP(Œ∏) = ùîº‚Çú[min(r‚Çú(Œ∏)A‚Çú, clip(r‚Çú(Œ∏), 1-Œµ, 1+Œµ)A‚Çú)]
   where r‚Çú(Œ∏) = œÄ(a‚Çú|s‚Çú;Œ∏)/œÄ(a‚Çú|s‚Çú;Œ∏_old)
   ```

2. **Soft Actor-Critic (SAC)**
   ```
   œÄ* = argmax_œÄ ùîºœÄ[Œ£‚ÇúŒ≥·µó(R(s‚Çú,a‚Çú) + Œ±H(œÄ(¬∑|s‚Çú)))]
   ```

## Exploration Strategies

### Basic Strategies
1. **Œµ-greedy**
   ```
   With probability 1-Œµ: Choose argmax‚ÇêQ(s,a)
   With probability Œµ: Choose random action
   ```

2. **Boltzmann Exploration**
   ```
   œÄ(a|s) = exp(Q(s,a)/œÑ)/Œ£‚Çê'exp(Q(s,a')/œÑ)
   ```

### Advanced Exploration
1. **UCB**
   ```
   Select a = argmax‚Çê[Q(s,a) + c‚àö(ln t/N(s,a))]
   ```

2. **Intrinsic Motivation**
   ```
   R'(s,a,s') = R(s,a,s') + Œ≤I(s,a,s')
   where I is intrinsic reward
   ```

## Multi-Agent RL

### Types of Environments
1. **Cooperative**
   ```
   Shared reward function
   Team-based objectives
   ```

2. **Competitive**
   ```
   Zero-sum games
   Nash equilibrium
   ```

3. **Mixed**
   ```
   Partial cooperation
   Individual and shared rewards
   ```

### Algorithms
1. **Independent Learning**
   ```
   Each agent learns independently
   Treats other agents as part of environment
   ```

2. **Centralized Training**
   ```
   Joint action space
   Shared value function
   Decentralized execution
   ```

## Practical Considerations

### Implementation Tips
1. **Hyperparameter Tuning**
   ```
   Learning rate
   Discount factor
   Exploration parameters
   Network architecture
   ```

2. **Stability Improvements**
   ```
   Gradient clipping
   Reward scaling
   Action noise
   Learning rate scheduling
   ```

### Common Issues
1. **Credit Assignment**
   ```
   Sparse rewards
   Delayed rewards
   Multi-agent credit assignment
   ```

2. **Sample Efficiency**
   ```
   Off-policy learning
   Prioritized replay
   Model-based methods
   ```

## Advanced Topics

### Hierarchical RL
1. **Options Framework**
   ```
   Option = (I, œÄ, Œ≤)
   I: Initiation set
   œÄ: Option policy
   Œ≤: Termination condition
   ```

2. **Feudal Networks**
   ```
   Manager-worker hierarchy
   Goal-directed behavior
   ```

### Meta-Learning
1. **Algorithm Adaptation**
   ```
   Learn learning algorithms
   Rapid adaptation to new tasks
   ```

2. **Transfer Learning**
   ```
   Knowledge transfer between tasks
   Policy distillation
   ```

### Safety in RL
1. **Constrained RL**
   ```
   Safety constraints
   Risk-sensitive objectives
   ```

2. **Robust RL**
   ```
   Uncertainty handling
   Adversarial training
   ```

## Performance Metrics

### Training Metrics
1. **Learning Curves**
   ```
   Average return
   Episode length
   Success rate
   ```

2. **Efficiency Metrics**
   ```
   Sample efficiency
   Wall-clock time
   Memory usage
   ```

### Evaluation Metrics
1. **Policy Quality**
   ```
   Final performance
   Stability
   Generalization
   ```

2. **Robustness**
   ```
   Parameter sensitivity
   Environmental variations
   Noise tolerance
   ```
